{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1dd5cfb4",
   "metadata": {},
   "source": [
    "# **The Molecular Treasure Hunt: Part pre3: 'The threequel' - training rescoring functions**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d177f9",
   "metadata": {},
   "source": [
    "*An &#8491;ngstrom sized adventure by Sarah Harris (Leeds Physics) and Geoff Wells (UCL Pharmacy)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bcfb1e9",
   "metadata": {},
   "source": [
    "## Training your rescoring functions!!\n",
    "\"*The chances of finding out what's really going on in the universe are so remote, the only thing to do is hang the sense of it and keep yourself occupied. DA*\"\n",
    "\n",
    "Unfortunately, there is no established relationship between the atomic structure of a complex and its binding affinity (yet). Therefore, a range of empirical scoring functions have been derived to estimate binding affinities (or rather free energies). Since we have no clear reason to favour one scoring function over others, we have chosen to use several in our analysis. Consequently, any trends that emerge may be more robust and warrant further investigation.\n",
    "\n",
    "In this notebook we use the Open Drug Discovery Toolkit to train some scoring functions for the next notebook. This utilizes a database of known protein ligand interactions and their affinities (the PDB Bind 2016 database in this case). We need to do this (unless we already have a copy of the trained scoring functions on our computer - you only need to do this once - a good thing because it takes some time!). If you are feeling adventurous/python-tastic and have experimental data for your system(s) then it is possible to use this to train the scoring functions too. Please note this is not a trivial exercise and some understanding of the underlying functions and python structure is essential!\n",
    "\n",
    "The NNScore is described in:\n",
    "JD Durrant, JA McCammon. NNScore 2.0: a neural-network receptor-ligand scoring function. J Chem Inf Model. 2011;51: 2897-2903. doi:10.1021/ci2003889\n",
    "JD Durrant, JA McCammon. BINANA: a novel algorithm for ligand-binding characterization. J Mol Graph Model. 2011;29: 888-893. doi:10.1016/j.jmgm.2011.01.004\n",
    "\n",
    "The PLECScoring methods are described in:\n",
    "M Wójcikowski, M Kukiełka, MM Stepniewska-Dziubinska, P Siedlecki. Development of a protein–ligand extended connectivity (PLEC) fingerprint and its application for binding affinity predictions Bioinformatics, 2019, 35, 1334–1341, doi:10.1093/bioinformatics/bty757\n",
    "\n",
    "The RFScoring methods are described in:\n",
    "PJ Ballester, JBO Mitchell. A machine learning approach to predicting protein-ligand binding affinity with applications to molecular docking Bioinformatics, 2010, 26, 1169-1175, doi:10.1093/bioinformatics/btq112\n",
    "\n",
    "We start by importing a number of python modules to run our calculations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "060069e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "#Importing glob is important for recursive filename searches - we use this to find all of our protein and ligand files!\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "from types import GeneratorType\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "from termcolor import colored\n",
    "import numpy as np\n",
    "\n",
    "import oddt\n",
    "from oddt.interactions import (close_contacts,\n",
    "                               hbonds,\n",
    "                               distance,\n",
    "                               halogenbonds,\n",
    "                               halogenbond_acceptor_halogen,\n",
    "                               pi_stacking,\n",
    "                               salt_bridges,\n",
    "                               pi_cation,\n",
    "                               hydrophobic_contacts)\n",
    "\n",
    "from oddt.scoring import scorer, ensemble_descriptor, ensemble_model\n",
    "from oddt.scoring.descriptors import (autodock_vina_descriptor,\n",
    "                                      fingerprints,\n",
    "                                      oddt_vina_descriptor)\n",
    "from oddt.scoring.models.classifiers import neuralnetwork\n",
    "from oddt.scoring.models import regressors\n",
    "from oddt.scoring.functions import rfscore, nnscore, PLECscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9dcd3cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training PLECscore rf with depths P5 L1 on PDBBind v2016\n",
      "Test set:\tR2_score: 0.5649\tRp: 0.7922\tRMSE: 1.4335\tSD: 1.3284\n",
      "Train set:\tR2_score: 0.9327\tRp: 0.9797\tRMSE: 0.4882\tSD: 0.3770\n",
      "OOB set:\tR2_score: 0.5090\tRp: 0.7193\tRMSE: 1.3182\tSD: 1.3069\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 24 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  92 out of 1000 | elapsed:   48.7s remaining:  8.0min\n",
      "[Parallel(n_jobs=-1)]: Done 193 out of 1000 | elapsed:  1.6min remaining:  6.5min\n",
      "[Parallel(n_jobs=-1)]: Done 294 out of 1000 | elapsed:  2.3min remaining:  5.6min\n",
      "[Parallel(n_jobs=-1)]: Done 395 out of 1000 | elapsed:  3.1min remaining:  4.7min\n",
      "[Parallel(n_jobs=-1)]: Done 496 out of 1000 | elapsed:  3.8min remaining:  3.9min\n",
      "[Parallel(n_jobs=-1)]: Done 597 out of 1000 | elapsed:  4.6min remaining:  3.1min\n",
      "[Parallel(n_jobs=-1)]: Done 698 out of 1000 | elapsed:  5.3min remaining:  2.3min\n",
      "[Parallel(n_jobs=-1)]: Done 799 out of 1000 | elapsed:  6.1min remaining:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 900 out of 1000 | elapsed:  6.8min remaining:   45.6s\n",
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed:  7.5min finished\n",
      "Test set:\tR2_score: 0.6361\tRp: 0.8163\tRMSE: 1.3110\tSD: 1.2575\n",
      "Train set:\tR2_score: 0.8081\tRp: 0.9082\tRMSE: 0.8741\tSD: 0.8351\n",
      "Training RFScore v3 on PDBBind v2016\n",
      "Test set:\tR2_score: 0.5664\tRp: 0.7966\tRMSE: 1.4310\tSD: 1.3161\n",
      "Train set:\tR2_score: 0.8889\tRp: 0.9576\tRMSE: 0.6650\tSD: 0.5748\n",
      "OOB set:\tR2_score: 0.5459\tRp: 0.7415\tRMSE: 1.3445\tSD: 1.3389\n"
     ]
    }
   ],
   "source": [
    "#We can train the rescoring models first (only need to do this if it has not been done before)...\n",
    "#The file size for the PLECscore_nn model is very large\n",
    "#Executing this on a small number of CPUs will take a long time!!!\n",
    "models = ([PLECscore(n_jobs=-1, version=v, size=65536)\n",
    "           for v in ['rf']] +\n",
    "          [nnscore(n_jobs=-1)] +\n",
    "          [rfscore(version=v, n_jobs=-1) for v in [3]])\n",
    "for model in models:\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b39148a",
   "metadata": {},
   "source": [
    "After this you should have a series of .pickle files that contain data on the trained scoring functions that will be used in the following steps. At the moment notebook 3 uses the PLEC rf score, the NNScore and the RFScore version 3. Again, if you already have these files then you don't need to run the this notebook!\n",
    "\n",
    "\"*Forty-two, said Deep Thought, with infinite majesty and calm. DA*\"\n",
    "\n",
    "Sarah and Geoff\n",
    "\n",
    "(an $O^{3}S$ production)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
